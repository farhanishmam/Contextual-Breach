<div align="center">

# Contextual Breach: Assessing the Robustness of Transformer-based QA Models

</div>

<p align="center">
    <a href="https://www.bubt.edu.bd/department/member_details/806"><strong>Talukder Asir Saadat*</strong></a>
    ·
    <a href="https://ishmamt.github.io/"><strong>Nahian Ibn Asad*</strong></a>
    ·
     <a href="https://farhanishmam.github.io/"><strong>Md Farhan Ishmam</strong></a>

</p>
<!-- 
<div align="center">

[![arXiv](https://img.shields.io/badge/arXiv-2409.10997-b31b1b.svg?logo=arxiv)](https://arxiv.org/abs/2409.10997)
![paper](https://img.shields.io/badge/Paper_Status-In--Review-yellow)
[![arXiv](https://img.shields.io/badge/Code-farhanishmam/Contextual--Breach-blue?logo=GitHub)](https://github.com/farhanishmam/Contextual-Breach/)
</div>
-->

**TLDR:** We evaluate language models on contextual question answering over perturbed contexts using seven categories of realistic textual perturbations, including character insertions, word swappings, and grammatical errors.

## Notebooks

- Generate the dataset using the `Generate 30k Dataset.pynb` notebook. Additionally, you can generate synthetic data using `Synthetic Data.ipynb`.
- The adversarial noises and models are available in the `Adversarial Noises.ipynb` and `Models/*.pynb` notebooks.
- A demo model can be run using the `Running model example.ipynb` notebook.
